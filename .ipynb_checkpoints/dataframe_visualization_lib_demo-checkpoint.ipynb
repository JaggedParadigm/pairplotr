{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data\n",
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../../library/dataframe_visualization_lib/')\n",
    "\n",
    "import dataframe_visualization_lib\n",
    "from dataframe_visualization_lib import continuous_pair_grid_vs_label\n",
    "from dataframe_visualization_lib import plot_label_versus_label\n",
    "from dataframe_visualization_lib import compare_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic-train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive title from name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                Braund, Mr. Owen Harris\n",
       "1      Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
       "2                                 Heikkinen, Miss. Laina\n",
       "3           Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
       "4                               Allen, Mr. William Henry\n",
       "5                                       Moran, Mr. James\n",
       "6                                McCarthy, Mr. Timothy J\n",
       "7                         Palsson, Master. Gosta Leonard\n",
       "8      Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n",
       "9                    Nasser, Mrs. Nicholas (Adele Achem)\n",
       "10                       Sandstrom, Miss. Marguerite Rut\n",
       "11                              Bonnell, Miss. Elizabeth\n",
       "12                        Saundercock, Mr. William Henry\n",
       "13                           Andersson, Mr. Anders Johan\n",
       "14                  Vestrom, Miss. Hulda Amanda Adolfina\n",
       "15                      Hewlett, Mrs. (Mary D Kingcome) \n",
       "16                                  Rice, Master. Eugene\n",
       "17                          Williams, Mr. Charles Eugene\n",
       "18     Vander Planke, Mrs. Julius (Emelia Maria Vande...\n",
       "19                               Masselmani, Mrs. Fatima\n",
       "20                                  Fynney, Mr. Joseph J\n",
       "21                                 Beesley, Mr. Lawrence\n",
       "22                           McGowan, Miss. Anna \"Annie\"\n",
       "23                          Sloper, Mr. William Thompson\n",
       "24                         Palsson, Miss. Torborg Danira\n",
       "25     Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...\n",
       "26                               Emir, Mr. Farred Chehab\n",
       "27                        Fortune, Mr. Charles Alexander\n",
       "28                         O'Dwyer, Miss. Ellen \"Nellie\"\n",
       "29                                   Todoroff, Mr. Lalio\n",
       "                             ...                        \n",
       "861                          Giles, Mr. Frederick Edward\n",
       "862    Swift, Mrs. Frederick Joel (Margaret Welles Ba...\n",
       "863                    Sage, Miss. Dorothy Edith \"Dolly\"\n",
       "864                               Gill, Mr. John William\n",
       "865                             Bystrom, Mrs. (Karolina)\n",
       "866                         Duran y More, Miss. Asuncion\n",
       "867                 Roebling, Mr. Washington Augustus II\n",
       "868                          van Melkebeke, Mr. Philemon\n",
       "869                      Johnson, Master. Harold Theodor\n",
       "870                                    Balkic, Mr. Cerin\n",
       "871     Beckwith, Mrs. Richard Leonard (Sallie Monypeny)\n",
       "872                             Carlsson, Mr. Frans Olof\n",
       "873                          Vander Cruyssen, Mr. Victor\n",
       "874                Abelson, Mrs. Samuel (Hannah Wizosky)\n",
       "875                     Najib, Miss. Adele Kiamie \"Jane\"\n",
       "876                        Gustafsson, Mr. Alfred Ossian\n",
       "877                                 Petroff, Mr. Nedelio\n",
       "878                                   Laleff, Mr. Kristo\n",
       "879        Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)\n",
       "880         Shelley, Mrs. William (Imanita Parrish Hall)\n",
       "881                                   Markun, Mr. Johann\n",
       "882                         Dahlberg, Miss. Gerda Ulrika\n",
       "883                        Banfield, Mr. Frederick James\n",
       "884                               Sutehall, Mr. Henry Jr\n",
       "885                 Rice, Mrs. William (Margaret Norton)\n",
       "886                                Montvila, Rev. Juozas\n",
       "887                         Graham, Miss. Margaret Edith\n",
       "888             Johnston, Miss. Catherine Helen \"Carrie\"\n",
       "889                                Behr, Mr. Karl Howell\n",
       "890                                  Dooley, Mr. Patrick\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n",
       "       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess',\n",
       "       'Jonkheer'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr          517\n",
       "Miss        186\n",
       "Mrs         125\n",
       "Child        40\n",
       "Dr            7\n",
       "Rev           6\n",
       "Military      5\n",
       "Noble         5\n",
       "Name: Title, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_map = {'Mr': 'Mr',\n",
    "'Miss': 'Miss',\n",
    "'Mrs': 'Mrs',\n",
    "'Master': 'Child',\n",
    "'Dr': 'Dr',\n",
    "'Don': 'Noble',\n",
    "'Rev': 'Rev',\n",
    "'Ms': 'Miss',\n",
    "'Mme': 'Miss',\n",
    "'Major': 'Military',\n",
    "'Col': 'Military',\n",
    "'Capt': 'Military',\n",
    "'the Countess': 'Noble',\n",
    "'Major': 'Military',\n",
    "'Mlle': 'Miss', \n",
    "'Jonkheer': 'Noble',\n",
    "'Lady': 'Noble',\n",
    "'Sir': 'Noble'} \n",
    "\n",
    "df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "df['Title'] = df.Title.map(title_map)\n",
    "\n",
    "df['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "Title          891 non-null object\n",
      "dtypes: float64(2), int64(5), object(6)\n",
      "memory usage: 90.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "Title          891 non-null object\n",
      "dtypes: float64(2), int64(5), object(6)\n",
      "memory usage: 90.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is missing Age, Cabin, and Embarked data. \n",
    "\n",
    "There is so much missing Cabin data it should probably just be discarded.\n",
    "\n",
    "Let's look at the distribution for Embarked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n",
      "S    0.724409\n",
      "C    0.188976\n",
      "Q    0.086614\n",
      "Name: Embarked, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print df['Embarked'].value_counts()\n",
    "print df['Embarked'].value_counts()/df['Embarked'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is such a high percentage of S cabins and only 3 entries don't have this data filling the blanks with S (highest frequency) is probably rather good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null object\n",
      "Title          891 non-null object\n",
      "dtypes: float64(2), int64(5), object(6)\n",
      "memory usage: 90.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df['Embarked'].fillna('S',inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 'Age' data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x10e37c410>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGNCAYAAADD1HGyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104621290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain median Age\n",
    "median_age = df['Age'].median()\n",
    "\n",
    "# Plot a quick histogram of it:\n",
    "df['Age'].plot.hist(bins=15,color='r')\n",
    "\n",
    "# Set title\n",
    "plt.title('Age distribution (median=%.1f)'%(median_age),size=15)\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel('Age',size=12)\n",
    "plt.ylabel('Number of passengers',size=12)\n",
    "\n",
    "# Add vertical median line\n",
    "plt.axvline(median_age,color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median age can be used to fill in the other data as a first stab.\n",
    "\n",
    "However, I'm interested in seeing if we can model the age instead. Let's go ahead and one-hot encode the varaibles other than age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 16 columns):\n",
      "Survived          891 non-null int64\n",
      "Pclass            891 non-null int64\n",
      "Age               714 non-null float64\n",
      "SibSp             891 non-null int64\n",
      "Parch             891 non-null int64\n",
      "Fare              891 non-null float64\n",
      "Sex_male          891 non-null uint8\n",
      "Embarked_Q        891 non-null uint8\n",
      "Embarked_S        891 non-null uint8\n",
      "Title_Dr          891 non-null uint8\n",
      "Title_Military    891 non-null uint8\n",
      "Title_Miss        891 non-null uint8\n",
      "Title_Mr          891 non-null uint8\n",
      "Title_Mrs         891 non-null uint8\n",
      "Title_Noble       891 non-null uint8\n",
      "Title_Rev         891 non-null uint8\n",
      "dtypes: float64(2), int64(4), uint8(10)\n",
      "memory usage: 50.5 KB\n"
     ]
    }
   ],
   "source": [
    "# df['Age'].fillna(median_age,inplace=True)\n",
    "simulation_df = df.copy()\n",
    "del simulation_df['PassengerId']\n",
    "del simulation_df['Cabin']\n",
    "del simulation_df['Ticket']\n",
    "del simulation_df['Name']\n",
    "simulation_df = pd.get_dummies(simulation_df,drop_first=True)\n",
    "simulation_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split the data for that has values (not null) for the Age feature so we can use it to train a model that we'll use to replace the N/A values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_df = simulation_df[pd.notnull(simulation_df['Age'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Dr</th>\n",
       "      <th>Title_Military</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Noble</th>\n",
       "      <th>Title_Rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Survived  Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  \\\n",
       "0          0       3  22.0      1      0   7.2500         1           0   \n",
       "1          1       1  38.0      1      0  71.2833         0           0   \n",
       "2          1       3  26.0      0      0   7.9250         0           0   \n",
       "3          1       1  35.0      1      0  53.1000         0           0   \n",
       "4          0       3  35.0      0      0   8.0500         1           0   \n",
       "6          0       1  54.0      0      0  51.8625         1           0   \n",
       "7          0       3   2.0      3      1  21.0750         1           0   \n",
       "8          1       3  27.0      0      2  11.1333         0           0   \n",
       "9          1       2  14.0      1      0  30.0708         0           0   \n",
       "10         1       3   4.0      1      1  16.7000         0           0   \n",
       "\n",
       "    Embarked_S  Title_Dr  Title_Military  Title_Miss  Title_Mr  Title_Mrs  \\\n",
       "0            1         0               0           0         1          0   \n",
       "1            0         0               0           0         0          1   \n",
       "2            1         0               0           1         0          0   \n",
       "3            1         0               0           0         0          1   \n",
       "4            1         0               0           0         1          0   \n",
       "6            1         0               0           0         1          0   \n",
       "7            1         0               0           0         0          0   \n",
       "8            1         0               0           0         0          1   \n",
       "9            0         0               0           0         0          1   \n",
       "10           1         0               0           1         0          0   \n",
       "\n",
       "    Title_Noble  Title_Rev  \n",
       "0             0          0  \n",
       "1             0          0  \n",
       "2             0          0  \n",
       "3             0          0  \n",
       "4             0          0  \n",
       "6             0          0  \n",
       "7             0          0  \n",
       "8             0          0  \n",
       "9             0          0  \n",
       "10            0          0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to split the data into input feature and output feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_feature = 'Age'\n",
    "column_names = list(age_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = age_df[input_features].copy()\n",
    "y = age_df[output_feature].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform a polynomial regression using degrees 1 to 5 and use GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('regressor', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])\n",
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('regressor', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))]),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'poly__degree': [1, 2, 3, 4]}, pre_dispatch='2*n_jobs',\n",
      "       refit=True, scoring='neg_mean_squared_error', verbose=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training L2 norm score:  126.873678544\n",
      "\n",
      "Test L2 norm score:  134.613914328\n",
      "\n",
      "Best parameters:\n",
      "\n",
      "{'poly__degree': 1}\n",
      "CPU times: user 412 ms, sys: 214 ms, total: 626 ms\n",
      "Wall time: 8.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# PolynomialFeature used with LinearRegression is a Polynomial regression\n",
    "pipeline = Pipeline([('poly',PolynomialFeatures()),('regressor', LinearRegression())])\n",
    "\n",
    "print pipeline\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'poly__degree': range(1,5)},\n",
    "                           cv=10, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid search using train data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Print scores\n",
    "print '\\nTraining L2 norm score: ',-grid_search.best_score_\n",
    "print '\\nTest L2 norm score: ', np.sqrt(np.square(y_test-y_pred).sum(axis=0))\n",
    "\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "# Here's how to predict\n",
    "# y_pred_pipe = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace null values with model prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cmshymansky/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/Users/cmshymansky/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "new_data = []\n",
    "for index, row in simulation_df.iterrows():\n",
    "    if pd.isnull(row[output_feature]):\n",
    "        X = row[input_features]\n",
    "         new_data.append(pipeline.predict(X.reshape(1,-1))[0])\n",
    "    else:\n",
    "        new_data.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Dr</th>\n",
       "      <th>Title_Military</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Noble</th>\n",
       "      <th>Title_Rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Survived, Pclass, Age, SibSp, Parch, Fare, Sex_male, Embarked_Q, Embarked_S, Title_Dr, Title_Military, Title_Miss, Title_Mr, Title_Mrs, Title_Noble, Title_Rev]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_df[pd.isnull(simulation_df['Age'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize distributions before and after replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x112b04e50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAANeCAYAAADgBMSLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xm4XVV9P/53CJIJDYpWoqlFRZfQ+i2KFq04gVpnqVVr\nJ+tYrUO/tVqsI4pa50ptxamOtcKvanHu1wEVRxwK1tLoihNiIIhUibFkEJLfH2tfODm5N7k5WeTm\nJq/X8/Dce/faZ+/POffcsN9nDXvB1q1bAwAAQB8HzHUBAAAA+xIhCwAAoCMhCwAAoCMhCwAAoCMh\nCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoKMD57oAgD2llPL8JC9KsjHJTWutP5vjknaolLIw\nyS+TXFBrvcWw7e5JPpPk3bXWR01wzAVJHpfkI7XWS2ax/58meXuSl9RaXzBs+2ySuyU5otb6/V2t\nYRbnvFGSh9Va3zCy7bNJ7prkVtfGOXdVKeXAJF9L8uGp12VvUkq5ZZLvJPlsrfX4Ydt2v8u9USnl\n95I8LcntkixKckGS9yX521rrFWP7Lk/y/CQPTbIiyZok70/yylrrT3fhnL+a5MVJjk9yaJLVSV5f\na/2nsf3+NMkrkty21vqTSZ4fsH/QkwXsT/40yS/SLtweM8e1TOqCJC9M8m8TPv70JG9KsnSW+39j\nON+nR7ZtHf7rbghY303yB2NNb0sLyLO+cL6WnZx2Mf63c13ILpjud7lXKaWcnOS9SY5K8q9JXp/k\niiTPSXJ2KWXJyL7XT3JOkr9K++DkjUm+muQvk5xTSlk5y3PebDjOI5OcleQf0v4+3lxKedXovrXW\ndyb5YdrfEMCM9GQB+4VSyj2T3CLJS5M8PcmfJfm7OS1qArXWHyY5ZTcOceNdPN9/JvnP3Tjfrlqa\n5LrT1PGuPVjDDpVSfiPJs5I8sda6ca7rma05+F3uklLKrZM8Ly3E3LHWetlI29vSPiQ5KS1sJ8kr\nk5S0Xq4/qrX+ctj3H5J8LskbkjxoFqc+NclhSe5fa/34cIyT03qMn15KeU+t9byR/Z+V5NOllAfU\nWj866fMF9m16soD9xWPTel/OTPLRJLcqpRw/tyUxjQVzXcAsPDfJ5Un+ea4L2cc8LO265NWjAWvw\ngrT3xoOSpJRyQJJHJNmc5M+nAlaS1FrPSfvd3L+UctSOTjj0Yj0kyRenAtZwjE1pvWcHJHni6GNq\nrZ9N8s20D2wApqUnC9jnlVKumzZn48e11nNLKacneXiSP880Q6dKKUem9RbdNcnBSb6S5NlpQ8OO\nr7UeMLb/vZP8dZI7JFmcNhfmXUn+vtZ65SxrvF3aUK7fTnJQkk+mfWI+vt+0c7JKKX+W5NFJbjM8\n/rtJzkjymlrrL0spv5bkB7lmmN93SykX1FpvUUp5dNpwvMckuX/ahez64fW5bmaex3OjUsrzkjw4\nbQjmfyR52ejF6sh5v1BrvdvYczlheJ7vqLU+dug9OHmo8bhSypYkL6y1njLTPLBSyp2H1+kuQ61r\n0oL0y0bn5JRSXph2oX6ntN/r45LcPG344YeSPH+aC/vtlFJukRYGThv93Y48z1OH470oyTFpQ90+\nmDak7YAkL0vyu0mWJTk/yXNrrZ8ZO8eiYf8/THLLtKFw5wzP6fPT1HR8WvA7JsmVw/nePM1+087J\nGt57zxhelxunzQP8TpJ3J3ltrXXr2HP8x7T31ilJ7jg8r68mOaXWevbIcd+e1vu0I1uT3LPW+rkk\nX0j7HX1qmv02DV+nejl/Zfj+v2ut/zPN/uel/T3cLcmqHZz/HmnhbbohlF9IC3HTfRjzjiSvKaWc\nUGs9awfHB/ZTQhawP/jDJEvShg8lrSfrp0keXEq5ca31x1M7llKOSbvgWprkA0m+l+R3knx2eMw2\nc5FKKc9I8qokP0mbcP8/Se4zbLvXMKRoy46KK6XcLcm/J1mYFhB+nOR+aUOedmpkQY9vpoWlrUnu\nmxYKb5vkj9J6Xl6YFqRuluR1w3PLyHN6eVq4et3wuC8Nz30m/5ZkS5J3ps1PeliSj5VSHlVr/ZfZ\n1D7mM0mWp82puTDJW5NMXbRvNw+slPKYJG9JCwUfTAtYd0kLDA8rpRxXa71o7PH/kOTX04aYfSgt\nUD4xLSDfcRY1/mFaqPjYDO33TPLUtPfYaUkekBbobpzk15JcJ21e3I3S5p19tJTy67XWHwzPaUna\n++/YJF8fjnFw2mv7mVLKY0eHTpZSHpnWa3NF2u/jirTQe98Z6ht/De+T5MNpcxWn3nsr04Lgq9PC\nzN+MHeO4tNfsS2nzoI4Y9r9LKeUutdb/GPY7My2U7cwFSTIErZne8w8fvk4Nd5wKXYtm2H95Wni6\nxU7Ofeu01+S74w211itLKT9KcvNSyoFjH5h8LG248e+nzeMC2IaQBewPpoYK/nOSDD07Z6T11Dw+\n2w77eXPaRe2JtdYPJ0kp5dlpAerEjFykllL+T9pKY+cnufvIaoV/U0p5y3Depyd5zUyFDav9vSXt\n3+N7TfVUDOf8UNqKaTvztLTAdPupQFdKeU7ap/mPLKU8Y1hJ8JRhbtrNkrxumlX6liT59bEeoJnO\nuSDJuiR3rrWuG/Y9Ncnnk/xjKeVDtdb1s6j9arXWz5VSfpghZNVaXzzTvsNqcKelhcfja63fHGk7\nJW1uz1uzbdhYkNYzdNuRUPOCtHB6+1LKnWutX95Jmfcavn51hvb/k+RptdbThuO/PC0wPiDttbn3\nyNyhC9MCzCPTeriS5CVJfiut1+q5I8/pRWk9qm8spXyq1nrx0EP7+rRg/Nu11m+PPKdPp80zGjc+\nHPPVaUH52Frr1UGjlPKyJP+d1hs0HrJ+M8kzaq2njuz/wrReqD/LMLyu1vqhtPfwbhmG9L0w7W/v\nDcOxf1ZK+W6SW5RSbjc6Z2r4m5r6W12+k8MfOnydaUGVdWmh+nqj+9RaV5dS1iW59y4/IWC/YE4W\nsE8bhv7dMcn5w8T/Ke9Mu+B8wnBRllLKb6YtG/2JqYCVJMNwqb9Kuxgd9cThGM+ZZjn4v067yHv8\nTkr8rSS3SnLm6FCwWuuG4ZyzcUCSGyb5jZHH/zItEFx/Nku1Dz69C8teb00bYrdu5JzfSFt17Xpp\nPRvXpj9JGxb5ytGANXhRWs/EvYchbqP+ZSpgJUmtdXOS/zf8ePNZnPeYJJfsYPn/9bmmxzTD6/mt\n4cfXjM4dShuOtmDqvMM8o8el9Yo+b/Sgtda1ab2ji9Kee9KC2/WTvHkqYA37/s/w+NnMb3tu2qIR\n2/TkDMf7cVqP27h1ab2doz4wfJ3NazhrpZQVST6RFobeODa08mVp7/0zSykPLKVct5RyRNpQ3VsP\n++zsNTho+Lpphvap7YunaTs/yc1KKTfcyTmA/ZCeLGBf97iM9GJNqbV+rZTy7bTVyR6Q5CNpgSdp\n818ytv8FpZQ1SX51ZPMdhq/3GYYZjlqQdsF961LK0vH7+4y43fD1a9Oc879KKbPpDXp92kX1eaWU\nb6TNc/pEks/Ndk7YYFfvP/WFabZ9KS0c3j7tYvfaMvW6fXa8odZ6VSnlS2m9VrdLW60uae+DOs2x\nLh++zjT0LElSSlmaNpfqezvY7ftTc5hG/GL4Ov64qffE1AV8SQuo65K8YJpexMPT3le3H34+Ou05\nbffeyfS/m+2M9NbeOG2I6C3SAsod04YKppSyYOw5fXeaIbDbvYallIcMNe7M22utF45vLO0F+Pe0\nYZYfTPIXY7W/fejRfF627TH7Vlov9btzzWs8kw3D14NmaJ96Pr+Ypm3qPlk3TrLT+XzA/kXIAvZZ\npd3M94+GH19ZSnnl2C5TF45PSgtZNxy2rZ3hkBdl25B1/eHrU3ZQxtZhv5ku9q4/7PPzGdp32rNU\naz25lFLThmr9dtqF7UlJflpKeXmt9dU7O8ZgZxeko7aOzmUbMRUKD96FY01iahjYuhnap+ZiLRvb\nPt2S61Pvg531ekz9vnf0Ok13Mb6jc093/JVpQ++ms3Vkv6mv2713huF0Oznd1UHm77LtsMoL0oY2\n/kaSQ9Jel9GQNdvX8MQks7lh9mfShlSO1nXPtDlm10vyL0keU2u9avyBw6Io7xrqv25a79In0uY0\nJjP/LU+Z+vs6ZIb25Wnv9en+Pv93+HqDnZwD2A8JWcC+7EFpnzKvzsw3YH18kt8Z5n2sT7tInGke\nx/j9m6YCxY1nszLdDP5nOOdMF3kHj5xnRrXW9yR5zzBP525pqwT+SZJXlFIuqrWePmF9M1lQSlky\nDGscddPh69TF69TF93TD08cD0K6Yuui9aabvnZoKID17GKae60y/q9019Xv+f7XWB8xi/6lV9bar\np5Sy09d26Jn7dNqQwJek9QZ9u9b6v0P7zgLKDtVaH5MJbvpdSvnDtAVcrpPkFbXW5+zkPBekLcAx\neow7pb33zt/J6b6da+bqjddxYNqHKtO9v5Jr3mPjfwMAQhawT5ta8OKltdZp72k0zPl4cFov0NQ8\nrDtPs98hacO5Rp2X1mt0p7SesNH9F6Wt1ndhrfW1O6hxaqjXXdPm3Iwe4+Zpc1FmDFmllJukzQ37\nfq31ncNiEx9NW7XunLS5Z3dPW9EuGVtdbjcdk+2Hpd0t2w5h2zx83e4Gw7lm3syo2dZ3btqy/HfP\n9AH6HpndRfas1Vp/Wkr537Qez2tDTbtgP7qUcp2x+Vsppdw1LTx/stb66bTXeEHae+e9Y8c6dhbn\nu1fawipvqrWePHauG+Wa+Vh77N5lpZRHpA0z3ZrkSbXWt+xg33elDfW9+WhP0zDH8uFpPU2f3ckp\nzx7OdXxa0Bx1t7RhhNstmz+Yeh/8cIZ2YD9m4QtgnzTMMblv2oXW+3ew61vSLiIfl7Zk9vlJHlhK\nuXrp8mFBgtekfbI+6q3DY19VShlfye2lSf5vpglso4ZV0c5Nu3HqiSPnvE52sCrhiPVpi2y8ZJoJ\n+FOfzo/OBZq6cJ9pDsqueOkQJpMkpZTjkvxxkkvS5tAkyaVpvVpHDveYmtr3RmnzZsZD1Wzre3da\ngPvLUso2835Ku3fXbdIWMLlougfvhm8kOXSa3/duGxbheHda8Hnl1IIsSVJKOTTtvXpSrpnD9e9p\nwyIfW0o5dmTf66UFhp0F1qkemJuNbhx+p2/ONdcI4+/7a0Up5VZpPVgLkjxqRwFr8N9pvXhPHdv+\n/LTFZP5+qlduJsP74xNJ7j7MIZuqZXHa3/DWtFUsx2tdmOSoJBfXWn8y3g6gJwvYV/1p2r9x793B\nohNJu1D9Udo8mIemha3PpvUEfSDtU+p75pqbwl59wVlr/XIp5cVpE+9XlVI+lLYi213Terd+kLaE\n+848Nq035v2llA+mzYm5T9on5TPN1ZqqYf2wZPcrkvx3KeXMJD9LW0r8vmlDJUdvTHth2kXs60sp\nZ9daT5lFfdNZkDYU85tDzVOv3y+T/PEQGFJr3VJKeWPazZy/NCydf520+z6dn+1Xo/tJ2ut8+1LK\n69J6bT48tk9qrReWUp6SNkzsnKGGNWmh9k5pwfIJ09S8o+czG59Im/d2tyT/OsvH7IqT0nqh/iLJ\nPUu7CfN1kvxeWs/SP9VaP5YktdaNpd0r7INJzi6lTN2n7YFpv4edPacvpN10+L6llLPTFi05JK13\n6LC038UN03pTe4fV6bwo7f50F6QtGHPyNPv8fKRn+HVpf+cvLqX8VtrQvzul/W4+k21vzTB1I+97\nJPlGrfWDI01/kfbc31dK+de099GJaff/mm71yqQtqHJwrp33ALAP0JMF7KsenWtulDujYdW0tw4/\nPqnW+rW0i+iPpQ0hemJacDoubcGD/x17/Mlpww2/Nnx9atqF6quS3Gk2PSnDRdyxSc5Iu5nuE9KC\n3z3SeqrGeyS2uTHvsLDF76etqvaQtPtM3TptQYOr72M1eGmSL6aFkaeUUg6e7pg7Ot9gS1r4/Eba\nUMsHpAWQu4wts520noXnpC1S8cS08PePQ83jz+XK4XgXD6/DiSPH2aaGWutb016jjyc5Ia1n7Ppp\nvTi3n+a131HPzmyHKf7LsO/9pmnb2Ws40/bR578u7f33wrSbU/9ZkkekLUn/qFrrE0cfXGv9VFqo\n/3jajaMflfZevPcM9Vy9bZhPd68k70kLu38x/PyV4ZhTy7Q/cBee4+4MR73v8PhfS1v4Y7r/rv7Q\nYqj/uLSgfdu0BWhukNaze/9a6/gCHfcYjvGQ0Y211u+khbP3pX248eS0BUweV2t99gy13n+odYf/\nvgD7rwVbt/Ycnj+ZYU7B69L+h31F2idDz661bi6lHJ42ROLOaZ9uPb3W+smRx94ryWvTlp39cpIn\njN4DBWC2hiF6N02bR7VlrG1xWuD5dq31tnNRH3uHUsrpaYuq3GSGVefYhw3DOH+QZE2t9bi5rgfY\nO+0tPVnvTxtjfpe0O98/KMmLh7YPpn2ieUzaWPUzSykrk2S4P8aZaZ9C3yFtFakPBGAyy9J6DL46\nzLkY9cy0noVPbvco9jenpN0/6dFzXAdz44Fpqw6+cI7rAPZic96TNdyjY1VGlkAupTwybajNo9JC\n1q9MdfuXUj6Z5PPDvTFOSXJcrfX4oW1J2oTrB9VaP7fnnw0w35VS3pG29Pm304ZgXZn2Ic490ub5\n3LHWevlMj2f/UEp5ddo92G5Va93RvbHYhwy9WN9IsrrW+vC5rgfYe+0NPVmXJLnvNPeYWZ42Rvrc\nsXHVX8g1q3Udm+TqMDWMzz43O1nNC2AHHps2t+fnaWHryWmrvb0iyR0ELAbPSZurt8N7OLHPeWza\nAiRP3NmOwP5tzlcXHCb5js6xWpA2cfystAubi8ce8uO0Vawyi3aAXTLMxXpztl2RD7YxrJ549E53\nZJ8yLLby1p3uCOz35jxkTeNVaUuj3jHJXyXZNNa+KW0sfNKWet1R+06VUi4f9t+tO9sDAADz3ook\nm2qth+zOQfaqkFVKeUXaErKPqLWuKqVsTFuOddSitBUIk3YvlfFAtSjtHjGztWjhwoWLV6xYMX6v\nFgDmiU1XbcpFP28rtt/0ejfNooVj/2vYsim5YljRfelNkwNm/VkcAPuRtWvX5qqrrtrt4+w1IauU\n8g9pY5z/qNY6tULgRWl3VB91WK7pdbpo+Hm8/bxdOPXaFStW3Pyss87axYoB2Ft8Zc1Xcqe33ilJ\ncsbjzsixK4/ddofLvpJ8orXnPmckNxxrB4AkJ5xwQtasWbPbI9z2hoUvMtzV/c+S/H6t9b0jTeck\nuX0pZfQjx+OG7VPtV9+jopSyNG2o4TkBAACYA3Pek1VKOTLJ85L8bZIvlVJuPNJ8dpIfJXlHKeXF\nSR6cNlfr0UP725I8s5RyUpKPJDk5yfdqrWfvofIBAAC2sTf0ZD04rY7npa0UeHHacMCLh1W+Tkwb\nAvj1JH+Y5MRa65okqbX+MMlD05ZU/WqSQ5L87p5+AgAAAFPmvCer1vqKtPvPzNT+vST33EH7x5Pc\n5looDQAAYJftDT1ZAAAA+wwhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoKM5X10QAJJk8+bNWbVq1USP\nXf3T1dd8v3p1Fl22aJv2pVeszq1H2q9Ys217D0cddVQOOuig7scFYP4RsgDYK6xatSpPftEZWbx8\nxS4/dt2iC5KbtO9P/f/OzfJNP92m/TbLL8hpx17T/u1127bvro3r1ua0kx+Zo48+uutxAZifhCwA\n9hqLl6/IskMP3+XHbV64+ervlyy/SZZdte0xliwdab/eTbLswF0/BwDMljlZAAAAHQlZAAAAHQlZ\nAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAA\nHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZ\nAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAA\nHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZ\nAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAA\nHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZ\nAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAA\nHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZ\nAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAA\nHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZ\nAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAA\nHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZ\nAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAAHQlZAAAA\nHQlZAAAAHQlZAAAAHQlZAAAAHR041wUA0NfmzZuzatWquS5jl61evXquSwCALoQsgH3MqlWr8uQX\nnZHFy1fMdSm7ZN1F/5XlN73tXJcBALtNyALYBy1eviLLDj18rsvYJRvWrZ3rEgCgC3OyAAAAOhKy\nAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAA\nOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKy\nAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAA\nOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKyAAAAOhKy\nAAAAOhKyAAAAOhKyAAAAOjpwrgsYV0pZlOTrSZ5Sa/3csO3vkzwtydYkC4avT6u1nja03yvJa5Pc\nIsmXkzyh1vqDOSgfAADYz+1VPVlDwDo9yVFjTUcmeVaSFUkOG76+bXjMryY5M8lbk9whyWVJPrCH\nSgYAANjGXtOTVUo5Msl7Zmg+Mskra62XTtP2+CRfq7WeOhznMUkuKaXcbaonDAAAYE/Zm3qy7p7k\nrCR3ThsSmCQppVw3yU2TrJ7hcXdKcnWYqrVuSHLucBwAAIA9aq/pyaq1vnHq+1LKaNORaXOwnldK\nuV+S/0nyd7XWdw3tK5JcPHa4HydZee1VCwAAML29qSdrJrdJsiXJqiT3S/JPSd5cSnnI0L40yaax\nx2xKsmiPVQgAADDYa3qyZlJrfVcp5UO11suHTeeXUm6d5M+TfDDJxmwfqBYl+dkeLBMAACDJ/OjJ\nykjAmvKttHlaSXJR2oqDow5LsvbargsAAGDcXh+ySikvKqV8cmzz7ZJ8e/j+nCTHjey/dGg/Z89U\nCAAAcI29frhgkg8n+ZtSyl+l3f/qd5L8cZJ7DO1vS/LMUspJST6S5OQk36u1nj0HtQIAAPu5vbUn\na+vUN7XWryd5WJJHJfmvJE9N8ge11q8O7T9M8tAkj03y1SSHJPndPV0wAABAspf2ZNVaF479/OG0\nHq2Z9v942iqEAAAAc2pv7ckCAACYl4QsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACA\njoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQs\nAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACA\njoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQs\nAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACA\njoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjoQs\nAACAjoQsAACAjoQsAACAjoQsAACAjoQsAACAjg6c6wIAYL7buuWqrF69eq7LmMhRRx2Vgw46aK7L\nANinCFkAsJs2rr80p55+aRYvXzvXpeySjevW5rSTH5mjjz56rksB2KcIWQDQweLlK7Ls0MPnugwA\n9gLmZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQk\nZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEA\nAHQkZAEAAHR04FwXAACzseWqK7Ph8jXTtm1YdHGybPh+3cU5aNNB27ZfefE13//84vzvum3bd9em\nX1x29fdLDlmZAxb63yvA/sz/BQCYFzZcviYnfOd9Wblk6XZt37vu5Tn3Ju37B1z6pdxy/apt2m+4\n6fKrv3/ApV/KsT/Ztr2L9T/Kmku/mLNu9bAsO/Tw/scHYN4QsgCYN1YuWZojlh283fZNizdfs8/i\npTliy7b7HDzWfsg0xwCAXszJAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA\n6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjI\nAgAA6Ei03DWKAAAgAElEQVTIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA\n6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjI\nAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA6EjIAgAA\n6EjIAgAA6OjAuS4AgLm35aors+HyNXNaw6ZfXLbD9g3r1u6hSgBg9whZAGTD5Wtywnfel5VLls5t\nIet/lKw/b9qm/7j8p8khN9jDBQHArhOyAEiSrFyyNEcsO3iuy5jRmg1XzHUJADAr5mQBAAB0JGQB\nAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0\nJGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0JGQBAAB0NFHI\nKqX8QSllUe9iAAAA5rtJe7L+OcklpZTTSim/1bMgAACA+WzSkHV4klcnOT7Jl0spq0opzyylHNat\nMgAAgHloopBVa11Ta31prfU2Se6a5PNJnpXkwlLKh0spDy2lHNizUAAAgPlgtxe+qLV+qdb6xCT3\nS/LFJA9I8r4kPyqlnFRKWbi75wAAAJgvdqu3qZTya0n+ZPjviCTfTfLsJB9NC1snJ/n1JH+6e2UC\nAADMDxOFrFLK49OC1V2SbEzy3iSPr7V+fmS380spN0zy5xGyAACA/cSkPVlvTvKVtAB1Rq11/Qz7\n/VeSN014DgAAgHln0pD1G7XWVaWUhbXWq5KklLIkyUG11nVTO9Va39WjSAAAgPli0oUvVpdS3pDk\nnJFtd0lyaSnl1aWU3V5QAwAAYD6aNAydkjYn6z0j285N8jdJnpDkpN2sCwAAYF6aNGT9UZJn1Fpf\nO7Wh1vrT4efnJnlcj+IAAADmm0lD1g2TfH+Gtm8nWTnhcQEAAOa1SUPWt5P83gxtD07ynQmPCwAA\nMK9NurrgqUneOdwH68wklya5UZIHJXlEkkd3qQ4AAGCemShk1Vr/uZRyvSTPT/LQkabLkjy11vrP\nPYoDAACYbyZear3W+vokK5IcmeS4JL+R5LBa6xs61QYAADDvTDpcMElSa92apHaqBQAAYN6bKGSV\nUm6UNi/rgUmWJVkwtsvWWutuBTgAAID5aNIg9I9pi1ycnmRNki3dKgLYx2zevDmrVq3aY+dbvXp1\nNqxbu0uP2dX9AYCZTRqy7pfkL2utb+5ZDMC+aNWqVfnoU56WlUuW7rFzPilJ1p836/3/4/KfJofc\n4FqrBwD2J5OGrF9m5psRAzBm5ZKlOWLZwXNdxozWbLhirksAgH3GpKsL/luSP+hZCAAAwL5g0p6s\nc5O8tJRyyyTnJBn/CHRrrfXFu1UZAADAPDRpyHr98PVuw3/jtiYRsgAAgP3ORCGr1jrxTYwBAAD2\nZbt9L6tSyvIkK5L8IMmVtdardrsqAACAeWriHqlSyj1KKV9J8tMk5yf59STvKaW8pldxAAAA881E\nIauUcnySTyTZkORZSRYMTf+Z5P+WUv6qT3kAAADzy6TDBV+a5AO11keUUg5M8sokqbX+bSnl4CSP\nT/J3kxy4lLIoydeTPKXW+rlh2+FJ3pLkzkkuSPL0WusnRx5zrySvTXKLJF9O8oRa6w8me2oAAACT\nm3S44NFJ3jZ8v3Ws7RNJDp/koEPAOj3JUWNNH0hycZJjkrw7yZmllJXDY341yZlJ3prkDkkuG/YH\nAADY4yYNWevSFruYzs2G9l1SSjky7Z5bNx/bfnxaD9UTa/PytN6qxw67PCHJ12qtp9Zav5XkMUkO\nL6VMt7Q8AADAtWrSkPXBtJsR32Fk29ahd+k5ST4ywTHvnuSstCGBC0a2H5vk3FrrxpFtXxj2m2r/\n3FRDrXVD2s2S7xwAAIA9bNI5WX+TFm6+kuSSYdvpSX41yYVJnr2rB6y1vnHq+1LKaNOKtKGCo36c\nZOUs2wEAAPaYiXqyaq0/SwtZT0rrRfpUkm8mOSnJMbXWy7pVmCxNsmls26Yki2bZDgAAsMdMfDPi\nWuumtBX/3tKvnGltTHKDsW2Lklwx0j4eqBYl+dm1XBcAAMB2JgpZpZRH7WyfWuu7Jjn2NC7K9qsN\nHpZk7Uj7YdO0n9fp/AAAALM2aU/WO2bYvjXJVUmuTNIrZJ2T5FmllEVD71mSHJfk8yPtx03tXEpZ\nmuR2SU7udH4AAIBZmzRk3XyabQcnuWvaohgnTlzR9s5O8qMk7yilvDjJg5PcMcmjh/a3JXlmKeWk\ntFUNT07yvVrr2R1rAAAAmJWJQlat9YczNP13KeWgJP+QFrgmdfUNjmutW0opD0m72fDXk3w3yYm1\n1jVTtZRSHprk75O8IMkXk/zubpwbAABgYhMvfLED30zy8t05QK114djP309yzx3s//Ekt9mdcwIA\nAPQw6c2IpzX0Yj0u7T5VAAAA+51JVxf8QUaG9A0WJrlhksVJnrmbdQEAAMxLkw4XPDvbh6ytSX6e\n5CO11k/tVlUAAADz1KQLXzy6cx0AAAD7hEmHC95sV/avtV44yXkAAADmm0mHC16Q7YcL7sjCne8C\nsPfYvHlzVq1a1eVYq1ev7nIcAGB+mDRkPSLJm5L8R5J3J7koyaFpNwr+/SQvSQtiAPPSqlWr8uQX\nnZHFy1fs9rE2rFubJ3WoCQCYHyYNWX+S5MPTzM3611LKpUnuUmt90W5VBjDHFi9fkWWHHt7nYOvP\n63McAGCvN+l9su6V5D0ztP17kuMmPC4AAMC8NmnIuizJsTO0nZA2fBAAAGC/M+lwwX9K8rxSyrIk\nH0rykyQ3TvLwJE9O8tQ+5QEAAMwvk4aslyQ5JMnTk/z1sG1BkiuSPKfW+qYOtQEAAMw7k96MeGuS\nZ5RSXpzkTklukDaE8Mu11vUd6wMAAJhXJu3JmvLzJBcP35+T5Dq7eTwAAIB5bdKFL1JK+eMkFyY5\nL8lHkhyR5B2llPeXUg7qVB8AAMC8MlHIKqU8Ism7knw6ySNHjnNmkvsneX6X6gAAAOaZSXuynpvk\njbXWRyX5t6mNtda3Jzk5yR90qA0AAGDemTRklbReq+l8JclNJzwuAADAvDZpyLo0yZEztB05tAMA\nAOx3Jg1ZZyQ5pZTysCSLhm1bSynHpM3Hem+P4gAAAOabSZdwf36S2yb51yRbhm2fTXJwks/HwhcA\nAMB+atKbEW9Kcr9Syr2THJ/k0CSXJzk7yceGmxUDAADsdyYKWaWUjyd5Za31k0k+2bckAACA+WvS\nOVl3yTXDBAEAABhMGrL+Pckfl1Ku07MYAACA+W7ShS82JvmTJI8opXwryS/G2rfWWk/YrcoAAADm\noUlD1sokXxz5ecFY+/jPAAAA+4VZh6xSykOTfLrWenmt9Z7XYk0AAADz1q7MyXpvkluPbiilnFRK\n+ZW+JQEAAMxfuzJccJshgKWUhUleluRTSS7tWRQAcO3buuWqrF69eq7LmNhRRx2Vgw46aK7LANjO\npHOypph7BQDz1Mb1l+bU0y/N4uVr57qUXbZx3dqcdvIjc/TRR891KQDb2d2QBQDMY4uXr8iyQw+f\n6zIA9imT3icLAACAaexqyNo6y20AAAD7pV0dLviBUsqmsW0fLqVsHtu2tdZ6y92oCwAAYF7alZD1\nzmm2nd2rEAAAgH3BrENWrfUx12YhAAAA+wILXwAAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQk\nZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEA\nAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQk\nZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEA\nAHR04FwXAAD7iiu3bsmGdWvnuoydWnLIyhyw0CUAwLXFv7AA0MklGzfmgRu/mJXrl851KTNas+GK\nnHWrh2XZoYfPdSkA+ywhCwA6WrlkaY5YdvBclwHAHDInCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMh\nCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAA\noCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMh\nCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAA\noCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMh\nCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAA\noCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMh\nCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAA\noCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoCMhCwAAoKMD57oAAGDP\nuXLrlmxYtzZJsukXl81xNTNbcsjKHLDQZQowP/nXCwD2I5ds3JgHbvxiVq5f2jas/1Gy/ry5LWrM\nmg1X5KxbPSzLDj18rksBmIiQBQD7mZVLluaIZQfPdRkA+yxzsgAAADoSsgAAADoSsgAAADoSsgAA\nADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoS\nsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAAADoSsgAA\nADoSsgAAADoSsgAAADoSsgAAADo6cK4LmI1SyolJ/i3J1iQLhq/vr7U+opRyeJK3JLlzkguSPL3W\n+sk5KhUAANjPzZeerKOSfCjJYcN/K5I8fmj7YJKLkxyT5N1JziylrJyLIgEAAOZFT1aSI5OcX2v9\nyejGUsrxSW6e5Nha68YkLy+lnJDksUlO2fNlAgB7wtYtV2X16tVzXcZEjjrqqBx00EFzXQZwLZov\nIeuoJNMNATw2yblDwJryhbShgwDAPmrj+ktz6umXZvHytXNdyi7ZuG5tTjv5kTn66KPnuhTgWjRf\nQlZJct9SynOTLEzy3iQvSBs2ePHYvj9OYrggAOzjFi9fkWWHHj7XZQBsZ68PWaWUmyVZkmRDkoen\nDQ983bBtaZJNYw/ZlGTRnqwRAABgyl4fsmqtF5ZSDq21Xj5s+mYpZWHaIhdvT3L9sYcsSnLFnqwR\nAABgyrxYXXAkYE35VpLFSS5JW21w1GFJ5tcAbQAAYJ+x14esUsp9SimXlVIWj2y+XZLLknw+yTGl\nlNHhgcclOWdP1ggAADBlrx8umORLacP//qmUckqSWyZ5ZZJXJPlckh8leUcp5cVJHpzkjkkePTel\nAgC768qtW7Jh3Y4HpWz6xWV7qJqZLTlkZQ5YOB8upYA9ba//l6HW+otSyu8kOTXJ15KsT/LGWutr\nkqSU8uAkb03y9STfTXJirXXNXNULAOyeSzZuzAM3fjEr1y/d8Y7rf5SsP2/PFDVmzYYrctatHmZ1\nQ2Bae33ISpJa67eS/M4Mbd9Pcs89WxEAcG1auWRpjlh28FyXATCRvX5OFgAAwHwiZAEAAHQkZAEA\nAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQk\nZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHQkZAEAAHR04FwXAPujLVu25Iy3vT1X/XLzXJeyQ8f89m/n\nqN/8zbkuA+D/b+/+ozy96vqAv3c3zv4sG7IU2HZ72OOJ3jJo2WJVqmAjqUJPY0DKKVFqQwJoGrAg\nUnsQJC1UC0hMSErw10HhlMYDKJCe8IcQaEyCWAz5IWc4d9BDlE0mWZd0lw3zY3/M9I/nOzpOdrM/\n5s48M995vc7JH9/neb53PjO5+8y8v/e59wKsKUIW9GBmZiZf+/gn8twNm/ou5Ql99sABIQsA4CwJ\nWdCTb9u4MVs2ru6QtSEb+i4BAGDNMScLAACgISELAACgISELAACgISELAACgISELAACgISELAACg\nISELAACgISELAACgISELAACgISELAACgISELAACgISELAACgISELAACgISELAACgISELAACgISEL\nAACgISELAACgISELAACgISELAACgISELAACgISELAACgISELAACgISELAACgISELAACgofP6LgBY\nvQ5+42DuvffeJbczOjqakZGRBhUBAKx+QhZwSp+/fyK3X3/7ktqYPjyRm665LPv27WtUFQDA6iZk\nAad03uYdGdm1t+8yAADWFHOyAAAAGhKyAAAAGhKyAAAAGhKyAAAAGhKyAAAAGrK6IADACpmbPZHx\n8fG+yzhn9j2EMyNkAQCskOkjB3L9zQeyZedE36WcNfsewpkTsgAAVtCWnbuz3R6EMNTMyQIAAGhI\nyAIAAGhIyAIAAGhIyAIAAGhIyAIAAGhIyAIAAGhIyAIAAGhIyAIAAGjIZsTAmnX06NGMjY0tS9vj\n4+OZOjzRpK1W7QAAa4OQBaxZY2NjufW1P5s9W7ctS/tXJcmRe5bczt2HHk3Ov2DJ7QAAa4OQBaxp\ne7Zuy4Xbd/RdxhPaPzXZdwkAwAoyJwsAAKAhIQsAAKAhIQsAAKAhIQsAAKAhIQsAAKAhqwsCAJyl\n43Oz57QH3sxjB5ehmlPbev6ebNzkzz1Yaf7VAQCcpYenp3PJ9F3Zc+Qc9uk78vUme/Cdzv6pydz2\nHS/L9l17l/1rAX+XkAUAcA7Wwj59QD/MyQIAAGhIyAIAAGhIyAIAAGhIyAIAAGhIyAIAAGjI6oLA\nspqbPZHx8fFlaXu52gUAWAohC1hW00cO5PqbD2TLzrPftPN0pg5P5KrmrQIALI2QBSy7LTt3L99m\nmCuwoScAwNkwJwsAAKAhIQsAAKAhIQsAAKAhIQsAAKAhIQsAAKAhIQsAAKAhIQsAAKAhIQsAAKAh\nIQsAAKAhIQsAAKAhIQsAAKAhIQsAAKCh8/ouAFidjs/OZnLycDZ844EltTPz2ME2BZ3E1OGJZWsb\nAOBcCVnAST0wNZl/OTuZPfs/tfTGjnw9OXLP0ttZ5O5DjybnX9C8XQCApRCygFPas3VbLty+o+8y\nTmn/1GTfJQAAPI45WQAAAA0JWQAAAA0JWQAAAA0JWQAAAA0JWQAAAA1ZXRAAYAgdn5ttup/g1OGJ\njI+PN2tv3ujoaEZGRpq3C30SsgAAhtDD09O5ZPqu7DmyrVmbU795T+5r1tpgK4733Zh9+/Y1bBX6\nJ2QBAAyp1b7fIQwrc7IAAAAaErIAAAAaErIAAAAaErIAAAAaErIAAAAasrogAAC9WY69txazFxcr\nTcgCAKA31938pWzd2W7T5MWmD0/kpmsusxcXK0rIAgCgN1t37s72XXv7LgOaMicLAACgISNZrCtH\njx7N2NhY32VkZmYmx44dSzb7JwgAMGz8hce6MjY2lqv/6+9ly87dvdYxe+JYvu+xmWTz1l7rAACg\nPSGLdWfLKnj2e/b40Wz4S//8AACGkTlZAAAADQlZAAAADQlZAAAADQlZAAAADZl5DwBAL47PzWbq\n8MSyfo2pwxMZHx9fcjujo6MZGRlpUBHrgZAFAEAvHp6eziXTd2XPkW3L+nWmfvOe3LeE9++fmkze\nd2P27dvXrCaGm5AFAEBv9mzdlgu37+i7DGjKnCwAAICGhCwAAICGhCwAAICGhCwAAICGhCwAAICG\nhCwAAICGhCwAAICGhCwAAICGhCwAAICGhCwAAICGhCwAAICGhCwAAICGhCwAAICGhCwAAICGhCwA\nAICGhCwAAICGhCwAAICGzuu7AAAAWO3Gx8d7+bqjo6MZGRnp5Wtz7oQsAAA4jetu/lK27pxY0a85\nfXgiN11zWfbt27eiX5elE7IAAOA0tu7cne279vZdBmuEOVkAAAANCVkAAAANCVkAAAANCVkAAAAN\nCVkAAAANCVkAAAANCVkAAAANCVkAAAANCVkAAAANCVkAAAANCVkAAAANCVkAAAANCVkAAAANCVkA\nAAANCVkAAAANCVkAAAANCVkAAAANCVkAAAANCVkAAAANCVkAAAANndd3AQAAwNIcPXo0Y2NjfZdx\nWqOjoxkZGem7jGUnZAEAwBo3NjaWW1/7s9mzdVvfpZzS/qnJ5H03Zt++fX2XsuyELAAAGAJ7tm7L\nhdt39F0GMScLAACgKSNZAACwCs3Nnsj4+PgZXXum17EyhCwAAFiFpo8cyPU3H8iWnROnvXbq8ESu\nWoGaODNCFgAArFJbdu7O9l17z+ziI/csay2cOXOyAAAAGjKSBQAAT+D43GymDp/+kb3WZh47eMbX\n9lEfpyZkAQDAE3h4ejqXTN+VPUd62IPqyNfP6DHAuw89mpx/wQoUxJkQsgAA4DRW+x5U+6cm+y6B\nBczJAgAAaEjIAgAAaEjIAgAAaEjIAgAAaEjIAgAAaMjqggAAwIoYHx/vu4QndPTo0SbtCFkAAMCK\nuO7mL2XrztW7cfLB//dYk3aGImSVUjYnuSnJS5NMJrm21vpr/VYFAAAstHXn7mzftbfvMk5pw8Y2\n8WgoQlaS9yR5TpKLkuxN8qFSygO11j/osygAAKBzfG42U4dX7yhWkszNHm/SzpoPWaWUbUleleSF\ntdb7ktxXSnl3ktclEbKWwdTUVK56w9uyZccFfZdy1qYOPZjkO/ouAwBg3Xl4ejqXTN+VPUe29V3K\nKd1wYjotHhhc8yErybPTfR9/vODYnUl+sZ9yht/MzEwOzjwpG3eM9l3KWZv71v5kS99VAACsT3u2\nbsuF23f0XcYpnbdhY5LZJbczDEu4705ysNa6cGzvkSRbSim7eqoJAABYp4ZhJGtbkplFx+Zfbz6D\n9++emJjIxRdf3LaqITY7O5sD3/hmNmz6VN+lnLW541OZnft8s0mNS6gkDx2bzJ2Prt7POY7PzuX4\n3OzgE53VaWb2RDZt2LCqa0zWRp1rvcZjG2ezt+5Nkvz60YfzbbMH/s75jZtms22sOz85+XBmTxzI\ncljrP8fVQo1tqLGdtVCnGts4fOJ40g3iLEnff2m2MJ3Hh6n515Nn8P6ZEydOZP/+/at7Fh5DZzLJ\n5OyJvss4A0sfMl9Wc3NZ9TUma6POtVzjiWTk2EiS5FuZffw1J5JHj44MXpzk/ErUuJqosQ01trEW\nakzWRp1qbGF3Hj+Ac9aGIWQ9mOQppZSNtdb5/2NPTzJVaz10ujfXWs9f1uoAAIB1ZfWO1Z25e5Mc\nS/LcBceen+SL/ZQDAACsZxvm5ub6rmHJSinvT/KDSa5MsifJ7ya5vNb6yT7rAgAA1p9heFwwSd6Y\n5KYkn01yOMkvCVgAAEAfhmIkCwAAYLUYhjlZAAAAq4aQBQAA0JCQBQAA0JCQBQAA0JCQBQAA0NCw\nLOF+1kopO5Ncm+SSdGHz1iRvqLUeHpy/IMlvJfmRJH+d5G211g/3VC5rXCllc7ptBl6aZDLJtbXW\nX+u3KoZBKeUfJLkhyQ+n61sfSfLmWuvRUsredPexf57kgSQ/V2v9dE+lMiRKKbcmeaTWeuXg9d7o\nZzRUShlJcl2Sn0gyk+QDtda3DM7tjf5GI6WUPUnen+SHknwjyXtrre8dnNubJfS19TyS9RtJvjvJ\ni5L8aJJnpvtBzvtgkr+X5PuT/HKS3y6l/LOVLpKh8Z4kz0lyUZKrk1xTSnlprxUxLH4/yZZ0G7Jf\nluTHkrxjcO6TSR5K8j1J/meSjw9+ocA5KaVcluRfLTr8iehntHVDkovTfdD9k0leU0p5zeCc+xot\nfTTJkXR/o70hyS+XUl48OLekvrYu98kqpWxLcijJD9Ra/3Rw7LlJ/ijJjiT/KMlXkzyj1vr1wfnf\nSrJp/pM7OFOD/nYwyQtrrXcMjr0lycW11hf0WhxrWimlJBlL8rRa68HBscuS/GqSf5/uF8RTa63T\ng3OfTnJHrfXtPZXMGlZKeXKS+9L90TFWa72ylPKCdCFLP6OJQT97JMkLaq13Do79QpLvTPLhuK/R\nSCnl/CSPJvmuWuvY4NjH0t3jPp4l9rX1OpI1m+4xwfsWHNuQZFO6kPV9Sf5qPmAN3JluuBDO1rPT\nPZr7xwuO3ZlulBSW4uEkL5oPWAvsTPLcJF+a/+Uw4D7GUrwnyYeSfGXBse+PfkZbz0tyaD5gJUmt\n9d211lfHfY22ppJ8K8kVpZTzBh9c/mCSe9Kgr63LkFVrna61/mGt9diCw69Pcn+t9dEku9Ol2IUe\nSWI4mnOxO8nBWuvxBcceSbKllLKrp5oYArXWwwufDy+lbEjyuiS3xX2MhgYjVs/P3z6KOk8/o7Vv\nT/JAKeWnSilfKaX8RSnlrYP7m/5GM7XWmXS/M69KF7i+kuRTtdbfSYO+NrQLX5RStiT5h6c4PVFr\nnVxw7euSvCzJCweHtqWbaLnQTJLNretkXThVf0r0Kdr61ST/NMn3Jnlj3MdoYLBwz68nubrWOtN9\n2Ps3/L6ktR3pHg386SSvTPfH7m+kW9hHf6O1Zya5Jd1I/XcnubGUclsa9LWhDVnpHmH4XJKTTTr7\n8XQ/0JRSrk7y3iSvr7XeNjg/ncf/EDen+wcOZ+tU/SnRp2iklPKuJP8xyb+ttY6VUqaTXLDoMvcx\nzhpT/5QAAAW8SURBVMV/SfLFWutnTnJOP6O14+kWHvuJWuv+JCmlPCPdolF/mGTxEyD6G+eklHJx\nklcl2TMY1bpnsLDFW9M9EbKkvja0IavWentO8zhkKeVNSd6d5Odrrf9jwakHkzx90eVPTzLRtEjW\niweTPKWUsrHWOjs49vQkU7XWQz3WxZAopdyY5GeSvKLW+onB4QeTjC661H2Mc/HyJE8rpRwZvN6c\nJKWUlyX5lehntDWRZHo+YA3UdI9pPZjkWYuu1984V89J8tVBwJp3T5JfTIO+ti7nZCVJKeXyJO9K\nN4J13aLTX0jyjMH+M/OeNzgOZ+veJMfSTaKc9/wkX+ynHIZJKeWadI/VvLzW+tEFp76Q5DmDR73m\nuY9xLv5Fusdonj3475Z0q249O8mfRD+jrS+km7N84YJjo+n2KfpCku/R32jkoSQXllIWDjo9M8nX\n0qCvrdcl3J+c5C+TfCzJmxedPlBrnSulfCrd3jOvT7fa4A1JfqjWeveKFstQKKW8P92KNVem+zTu\nd5NcXmv9ZJ91sbaVUp6Z5P50owk3LTr91+lWUP1yusUKLk13v3vWok+I4ayUUn4nydxgCfeN0c9o\nrJRyS7rHUK9ONyfrQ0nenm7T2PuT/Fn0N5aolPKkdItdfDrdnrj/OMkH0vWpD2SJfW29jmT9aJLt\nSS5Pl2IfSjf891C6PbIyOPfNdIn1zUmuELBYgjcmuTvJZ5PcmOSXBCwauDTdffytWXQvGzya+pJ0\njzf8aboNPV/iDxFaGvSzF0c/o61XJPnzJHek+1Dyhlrr+wb97dLobzRQa/1muk2vdyf5v0muTfL2\nWutvt+hr63IkCwAAYLms15EsAACAZSFkAQAANCRkAQAANCRkAQAANCRkAQAANCRkAQAANCRkAQAA\nNCRkAQAANCRkAQAANCRkATD0SikfLqXMllJ+ru9aABh+QhYAQ62U8qQkL0lyf5Kf7rkcANYBIQuA\nYfeTSeaSvD5JKaX8cM/1ADDkzuu7AABYZlck+Uyt9fZSyp8n+Zkkn1t4QSnlTUn+Q5LdSe5O8q4k\ntyS5qNb6R4NrvivJO5M8f/C225L8fK31ayvyXQCwZhjJAmBolVKeleR7k3xwcOiDSV5SSvn7C655\nW7rw9HtJLk3yJ0k+km70a/6a70xyV5KnJPmpJFcm+fYkd5VSnrL83wkAa4mQBcAwuzLJwST/e/D6\ng+me4nhVkpRStiX5z0lurLW+pdb6mVrrm/K3oWzeNUm+leTiWusttdbfT3JRkq1J/tOyfxcArClC\nFgBDqZRyXpJXJPlEku2llJ1JHktyZ5LXDC77gSRbknxs0dtvTrJhwesXJPk/SaZLKZtKKZsGbd2R\n5EeW63sAYG0yJwuAYfVjSZ6abtTq1QuOzyVJKeWFSZ48OHZg0XsfWfR6V5KXJ7ls0fG5k7wXgHVO\nyAJgWF2R5C/SPTK4cFRqQ7rRrauSXDt4/bQkX11wzVMXtXUoyaeTvGdRW0lyvF3JAAwDIQuAoVNK\neVqSFyV5Z631jpOc/2iSV6Zb1v1wkh9P9xjhvH+TBQtfJLk9yWiS+2qtswva+V9Jaro9uAAgiZAF\nwHC6PMmmdCsGnsyH0j1CeEW65drfUUqZSjfv6qJ0o1xJMh+o3p7k80luLaW8P8lMuqXgL00XyADg\nb1j4AoBh9MokX661jp3sZK31ziRfS/co4buSvC3Jv0u3CuHzkvzC4NLHBtf/Wbr9sWbTBbSPpHvE\n8MW11k8u23cBwJq0YW5u7vRXAcAQKqVsTLcC4edqrfsXHH9tkuuT7Kq1frOv+gBYm4QsANa1UsqX\n0z3+99/S7an1T5K8I8kf1Fpf/UTvBYCTMScLgPXuXyf570luSnJ+kr9Kt+rgO/ssCoC1y0gWAABA\nQxa+AAAAaEjIAgAAaEjIAgAAaEjIAgAAaEjIAgAAaEjIAgAAaEjIAgAAaEjIAgAAaOj/A1d75BHG\nCuQGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e5e6250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain median Age\n",
    "median_age = simulation_df['Age'].median()\n",
    "\n",
    "# Plot a quick histogram of it:\n",
    "simulation_df['Age'].plot.hist(bins=15,color='b',figsize=(10,10))\n",
    "\n",
    "# Set title\n",
    "plt.title('Age distribution (median=%.1f)'%(median_age),size=15)\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel('Age',size=12)\n",
    "plt.ylabel('Number of passengers',size=12)\n",
    "\n",
    "# Add vertical median line\n",
    "plt.axvline(median_age,color='orange')\n",
    "\n",
    "# Obtain median Age\n",
    "median_age = df['Age'].median()\n",
    "\n",
    "# Plot a quick histogram of it:\n",
    "df['Age'].plot.hist(bins=15,color='r')\n",
    "\n",
    "\n",
    "# Add vertical median line\n",
    "plt.axvline(median_age,color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifted the median just a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulation_df['Survived'].value_counts().values/float(simulation_df['Survived'].value_counts().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null accuracy of ~62% if always predict death."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to flexibly use different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model(X,y,\n",
    "               scale_type=None,\n",
    "               feature_selection_type='select_k_best',\n",
    "               estimator='knn',\n",
    "               param_dist={},\n",
    "               use_default_param_dist=True,\n",
    "               n_jobs=-1,\n",
    "               num_parameter_combos=[],\n",
    "               cv=10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set estimator options\n",
    "    estimator_options = ['knn']\n",
    "    \n",
    "    # Set classifiers\n",
    "    classifiers = ['knn']\n",
    "    \n",
    "    # Set regressors\n",
    "    regressors = []\n",
    "    \n",
    "    # Initialize pipeline steps\n",
    "    pipeline_steps = []\n",
    "    \n",
    "    # Add scaling step\n",
    "    if scale_type:\n",
    "        if scale_type == 'standard':\n",
    "            pipeline_steps.append(('scaler', StandardScaler()))\n",
    "            \n",
    "    # Add feature selection step\n",
    "    if feature_selection_type:\n",
    "        if feature_selection_type == 'select_k_best':\n",
    "            pipeline_steps.append(('feature_selection', SelectKBest(f_classif)))\n",
    "            \n",
    "    # Add estimator\n",
    "    if estimator in estimator_options:\n",
    "        if estimator == 'knn':\n",
    "            pipeline_steps.append(('estimator', KNeighborsClassifier()))\n",
    "    else:\n",
    "        error = 'Estimator %s is not recognized. Currently supported estimators are:\\n'%(estimator)\n",
    "\n",
    "        for option in estimator_options:\n",
    "            error += '\\n%s'%(option)\n",
    "\n",
    "        raise Exception(error)\n",
    "\n",
    "    # Add default scaling step to grid parameters\n",
    "    if use_default_param_dist:\n",
    "        # Add default feature selection parameters\n",
    "        if feature_selection_type:\n",
    "            if feature_selection_type == 'select_k_best' and 'feature_selection__k' not in param_dist:\n",
    "                param_dist['feature_selection__k'] = range(1,len(X.columns)+1)\n",
    "        \n",
    "        # Add default estimator parameters\n",
    "        if estimator == 'knn':\n",
    "            # Add default number of neighbors for k-nearest neighbors\n",
    "            if 'estimator__n_neighbors' not in param_dist:\n",
    "                param_dist['estimator__n_neighbors'] = range(1,31)\n",
    "                \n",
    "            # Add default point metric options for for k-nearest neighbors\n",
    "            if 'estimator__weights' not in param_dist:\n",
    "                param_dist['estimator__weights'] = ['uniform','distance']\n",
    "\n",
    "    # Form pipeline\n",
    "    pipeline = Pipeline(pipeline_steps)\n",
    "    \n",
    "    # Set scoring\n",
    "    if estimator in classifiers:\n",
    "        scoring = 'accuracy'\n",
    "    elif estimator in regressors:\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    \n",
    "    # Initialize full or randomized grid search\n",
    "    if num_parameter_combos:\n",
    "        grid_search = RandomizedSearchCV(pipeline,\n",
    "                                         param_dist,\n",
    "                                         cv=cv,scoring=scoring,\n",
    "                                         n_iter=num_parameter_combos, # n_iter=10 means 10 random parameter combinations tried\n",
    "                                         n_jobs=n_jobs)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(pipeline, \n",
    "                                   param_dist,\n",
    "                                   cv=cv, scoring=scoring,n_jobs=n_jobs)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Perform grid search using above parameters\n",
    "    grid_search.fit(X_train,y_train)\n",
    "\n",
    "    # Make prediction based on model\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Print scores\n",
    "    if estimator in classifiers:\n",
    "        # Calculate confusion matrix\n",
    "        conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Print training and test scores\n",
    "        print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "        print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "        \n",
    "        # Print out confusion matrix and normalized confusion matrix containing probabilities\n",
    "        print 'Confusion matrix: \\n\\n',conf_mat\n",
    "        print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "        # Print out classification report\n",
    "        print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)        \n",
    "    elif estimator in regressors:\n",
    "        print '\\nTraining L2 norm score: ',-grid_search.best_score_\n",
    "        print '\\nTest L2 norm score: ',np.sqrt(np.square(y_test-y_pred).sum(axis=0))\n",
    "    \n",
    "    # Print best parameters\n",
    "    print '\\nBest parameters:\\n'\n",
    "    print grid_search.best_params_\n",
    "\n",
    "    # Fit pipeline with best parameters obtained from grid search using all data\n",
    "    pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "    # Print pipeline\n",
    "    print '\\n',pipeline\n",
    "    \n",
    "    # Return pipeline\n",
    "    return pipeline # Use pipeline.predict() for productionalization\n",
    "        \n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Set parameters to perform a grid search over\n",
    "param_dist = {\n",
    "#     'feature_selection__k': range(1,len(input_features)),\n",
    "#     'estimator__n_neighbors': range(1,31),\n",
    "#     'estimator__weights': ['uniform']\n",
    "}\n",
    "\n",
    "# Figure out best model\n",
    "pipeline = train_model(X,y,\n",
    "           scale_type='standard',\n",
    "           feature_selection_type='select_k_best',\n",
    "           estimator='knn',\n",
    "           param_dist=param_dist,\n",
    "           num_parameter_combos=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try_models(X,y,\n",
    "           scale_type='standard',\n",
    "           feature_selection_type='select_k_best',\n",
    "           estimator='knn',\n",
    "           param_dist=param_dist,\n",
    "           num_parameter_combos=122*6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/(9.42/60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, feature select, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif)), \n",
    "        ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'feature_selection__k': range(1,len(input_features)),\n",
    "                            'classifier__n_neighbors': range(1,31),\n",
    "                           'classifier__weights': ['uniform','distance']},\n",
    "                           cv=10, scoring='accuracy',n_jobs=-1)\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, feature select, KNN w/ RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif)), \n",
    "        ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "# Set parameters to iterate over\n",
    "param_dist = {\n",
    "    'feature_selection__k': range(1,len(input_features)),\n",
    "    'classifier__n_neighbors': range(1,31),\n",
    "    'classifier__weights': ['uniform','distance']\n",
    "}\n",
    "\n",
    "# n_iter=10 means 10 random parameter combinations tried\n",
    "grid_search = RandomizedSearchCV(pipeline,\n",
    "                          param_dist,\n",
    "                          cv=10,scoring='accuracy',n_iter=800,n_jobs=-1)\n",
    "\n",
    "# # Initialize grid search with pipeline\n",
    "# grid_search = GridSearchCV(pipeline, \n",
    "#                            param_dist,\n",
    "#                            cv=10, scoring='accuracy',n_jobs=-1)\n",
    "# # Print grid_search parameters\n",
    "# print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid search using above parameters\n",
    "grid_search.fit(X_train,y_train)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, feature select, logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif)), \n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, {\n",
    "        'feature_selection__k': range(1,len(input_features)),\n",
    "        'classifier__C': np.logspace(-10,10,5)    \n",
    "    },\n",
    "    cv=10, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, feature select, support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif)), \n",
    "        ('classifier', SVC())\n",
    "    ])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, {\n",
    "        'feature_selection__k': range(1,len(input_features))\n",
    "    },\n",
    "    cv=10, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, PCA, feature select, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Unsupervised learning tools\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('unsupervised', PCA()),        \n",
    "        ('feature_selection', SelectKBest(f_classif)),         \n",
    "        ('classifier', SVC())\n",
    "    ])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, {\n",
    "#         'feature_selection__k': range(1,len(input_features))\n",
    "#         'unsupervised__n_components': range(1,5)\n",
    "    },\n",
    "    cv=10, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale, SelectKBest, Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Unsupervised learning tools\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "#         ('feature_selection', SelectKBest(f_classif)),         \n",
    "        ('classifier', MLPClassifier(solver='lbfgs',alpha=1e-5))\n",
    "    ])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, {\n",
    "        'classifier__hidden_layer_sizes': [[x] for x in range(1,100)]\n",
    "#         'feature_selection__k': range(1,len(input_features))\n",
    "#         'unsupervised__n_components': range(1,5)\n",
    "    },\n",
    "    cv=10, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Unsupervised learning tools\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif)),         \n",
    "        ('classifier', ExtraTreesClassifier(max_depth=4,learning_rate=0.01,min_samples_split=1))\n",
    "    ])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, {\n",
    "        'classifier__n_estimators': [300],\n",
    "        'classifier__loss' : [ 'ls', 'lad' ],\n",
    "        'feature_selection__k': range(1,len(input_features))\n",
    "#         'unsupervised__n_components': range(1,5)\n",
    "    },\n",
    "    cv=10, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Set output feature\n",
    "output_feature = 'Survived'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Regression tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Unsupervised learning tools\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "# Cross validation tools\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif)),         \n",
    "        ('classifier', GradientBoostingRegressor())\n",
    "    ])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, {\n",
    "        'classifier__n_estimators': [x for x in range(190,210)],\n",
    "        'feature_selection__k': range(1,len(input_features))\n",
    "#         'unsupervised__n_components': range(1,5)\n",
    "    },\n",
    "    cv=10, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one apparently can't handle a mix of binary and continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "dataframe_visualization_lib.continuous_pair_grid_vs_label(simulation_df,hue_feature='Survived',\n",
    "                              plot_medians=True,plot_vars=['Age','Fare'],plot_means=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete remaining fields with too much missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I delete the Cabin data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df['Cabin']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create classfier for survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim data for machine learning packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trimmed_df = df.copy()\n",
    "\n",
    "del trimmed_df['Name']\n",
    "del trimmed_df['Ticket']\n",
    "del trimmed_df['PassengerId']\n",
    "\n",
    "print trimmed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(trimmed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the output feature name and the input feature names and split into training and target data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_feature = 'Survived'\n",
    "column_names = list(encoded_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [column_name for column_name in column_names if column_name not in ['Survived','Sex_male','Embarked_S']]\n",
    "\n",
    "# Split into features and responses\n",
    "X = encoded_df[input_features].copy()\n",
    "y = encoded_df[output_feature].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out null accuracy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_df['Survived'].value_counts().values/float(encoded_df['Survived'].value_counts().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most died. So a null model of always predicting death would result in a 62% null accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN w/ SelectKBest feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([('kbest', SelectKBest(f_classif)), ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'kbest__k': range(1,len(input_features)),\n",
    "                            'knn__n_neighbors': range(1,31),\n",
    "                           'knn__weights': ['uniform','distance']},\n",
    "                           cv=10, scoring='accuracy',n_jobs=-1)\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([('kbest', SelectKBest(f_classif)), ('lr', LogisticRegression())])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'kbest__k': range(1,len(input_features)),'lr__C': np.logspace(-10,10,5)},\n",
    "                           cv=10, scoring='accuracy') #,n_jobs=-1\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([('kbest', SelectKBest(f_classif)), ('svm', SVC())])\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'kbest__k': range(1,len(input_features))},\n",
    "                           cv=10, scoring='accuracy') #,n_jobs=-1\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# Form pipeline\n",
    "pipeline = Pipeline([('kbest', SelectKBest(f_classif)), ('nb', MultinomialNB())])\n",
    "\n",
    "print pipeline\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'kbest__k': range(1,len(input_features)),'nb__alpha': np.arange(0.0,1.0,0.1)},                           \n",
    "                           cv=10, scoring='accuracy') #,n_jobs=-1\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale data then use classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# rescaled_X = StandardScaler().fit(X).transform(X)        \n",
    "\n",
    "# Pipeline([('scaler', StandardScaler()), ('clf', LinearSVC())])\n",
    "# Form pipeline \n",
    "pipeline = Pipeline([('kbest', SelectKBest(f_classif)),\n",
    "                     ('scaler', StandardScaler()), \n",
    "                     ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "# pipeline = Pipeline([('scaler', MinMaxScaler()), ('classifier', KNeighborsClassifier())])\n",
    "# pipeline = Pipeline([('scaler', Normalizer()), ('classifier', KNeighborsClassifier())])\n",
    "# pipeline = Pipeline([('scaler', Binarizer()), ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "print pipeline\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'kbest__k': range(1,len(input_features)),                            \n",
    "                            'classifier__n_neighbors': range(1,31),\n",
    "                           'classifier__weights': ['uniform','distance']},\n",
    "                           cv=10, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "# rescaled_X = StandardScaler().fit(X).transform(X)        \n",
    "\n",
    "# Pipeline([('scaler', StandardScaler()), ('clf', LinearSVC())])\n",
    "# Form pipeline \n",
    "pipeline = Pipeline([('kbest', SelectKBest(f_classif)),\n",
    "                     ('scaler', StandardScaler()), \n",
    "                     ('classifier', RandomForestClassifier())])\n",
    "\n",
    "pipeline = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "# pipeline = Pipeline([('scaler', MinMaxScaler()), ('classifier', KNeighborsClassifier())])\n",
    "# pipeline = Pipeline([('scaler', Normalizer()), ('classifier', KNeighborsClassifier())])\n",
    "# pipeline = Pipeline([('scaler', Binarizer()), ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "print pipeline\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'classifier__n_estimators': range(90,100)},\n",
    "                           cv=10, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "print y\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "# rescaled_X = StandardScaler().fit(X).transform(X)        \n",
    "\n",
    "# Pipeline([('scaler', StandardScaler()), ('clf', LinearSVC())])\n",
    "# Form pipeline \n",
    "# pipeline = Pipeline([('kbest', SelectKBest(f_classif)),\n",
    "#                      ('scaler', StandardScaler()), \n",
    "#                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# pipeline = Pipeline([('classifier', RandomForestClassifier())])\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),('classifier', MLPClassifier(solver='lbfgs',alpha=1e-5))]) #, alpha=1e-5,hidden_layer_sizes=(5, 2)\n",
    "\n",
    "\n",
    "# pipeline = Pipeline([('scaler', MinMaxScaler()), ('classifier', KNeighborsClassifier())])\n",
    "# pipeline = Pipeline([('scaler', Normalizer()), ('classifier', KNeighborsClassifier())])\n",
    "# pipeline = Pipeline([('scaler', Binarizer()), ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "print pipeline\n",
    "\n",
    "# Initialize grid search with pipeline\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'classifier__hidden_layer_sizes': [[x] for x in range(1,100)]},\n",
    "                           cv=10, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_\n",
    "\n",
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize effect of each feature on Survival\n",
    "## PassengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['PassengerId'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply the passenger id and I wouldn't expect it to correlate with anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of people that didn't survive are about twice as higher as those who did. This could cause problems with cross validation later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "input_label_feature = 'Pclass'\n",
    "output_label_feature = 'Survived'\n",
    "output_labels = ['0','1']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "dataframe_visualization_lib.plot_label_versus_label(df,input_label_feature,\n",
    "                                                    output_label_feature,\n",
    "                                                    output_labels=output_labels,\n",
    "                                                    colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more of Pclass 1 survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name is just an alternate id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "input_label_feature = 'Sex'\n",
    "output_label_feature = 'Survived'\n",
    "output_labels = ['0','1']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "dataframe_visualization_lib.plot_label_versus_label(df,input_label_feature,\n",
    "                                                    output_label_feature,\n",
    "                                                    output_labels=output_labels,\n",
    "                                                    colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Survived'].value_counts().sort_index().index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much larger proportion of the women survived than the men!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "input_feature = 'Age'\n",
    "output_label_feature = 'Survived'\n",
    "output_labels = ['0','1']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "dataframe_visualization_lib.plot_label_vs_continuous(df,input_feature,\n",
    "                                                     output_label_feature,\n",
    "                                                     output_labels=output_labels,\n",
    "                                                     colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be no difference between the median ages of those who survived and those who didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['SibSp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "input_label_feature = 'SibSp'\n",
    "output_label_feature = 'Survived'\n",
    "output_labels = ['0','1']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "dataframe_visualization_lib.plot_label_versus_label(df,input_label_feature,\n",
    "                                                    output_label_feature,\n",
    "                                                    output_labels=output_labels,\n",
    "                                                    colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most people are in the Sibsp category.\n",
    "\n",
    "Doesn't appear to much of a pattern here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Parch'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "input_label_feature = 'Parch'\n",
    "output_label_feature = 'Survived'\n",
    "output_labels = ['0','1']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "dataframe_visualization_lib.plot_label_versus_label(df,input_label_feature,\n",
    "                                                    output_label_feature,\n",
    "                                                    output_labels=output_labels,\n",
    "                                                    colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are way more of Parch 0. I suspect these were the cheaper tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that there are more than 1 people per 'Ticket'. Maybe these are rooms.\n",
    "\n",
    "Doesn't look interesting at face value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Fare'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "input_feature = 'Fare'\n",
    "output_label_feature = 'Survived'\n",
    "output_labels = ['0','1']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "dataframe_visualization_lib.plot_label_vs_continuous(df,input_feature,\n",
    "                                                     output_label_feature,\n",
    "                                                     output_labels=output_labels,\n",
    "                                                     colors=colors,plot_quantiles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there might be a trend. Looks like those that paid more survived more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "input_label_feature = 'Embarked'\n",
    "output_label_feature = 'Survived'\n",
    "output_labels = ['0','1']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "dataframe_visualization_lib.plot_label_versus_label(df,input_label_feature,\n",
    "                                                    output_label_feature,\n",
    "                                                    output_labels=output_labels,\n",
    "                                                    colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataframe_visualization_lib\n",
    "reload(dataframe_visualization_lib)\n",
    "input_label_feature = 'Title'\n",
    "output_label_feature = 'Survived'\n",
    "output_labels = ['0','1']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "dataframe_visualization_lib.plot_label_versus_label(df,input_label_feature,\n",
    "                                                    output_label_feature,\n",
    "                                                    output_labels=output_labels,\n",
    "                                                    colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way more of S in general and more of them died. Class C survived the most. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`**Perform one-hot enconding for variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(trimmed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure out null-accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_df['Survived'].value_counts().values/float(encoded_df['Survived'].value_counts().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the null accuracy is ~62% if we always predict death."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into inputs and outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = list(encoded_df.columns)\n",
    "output_feature = 'Survived'\n",
    "input_features = [column_name for column_name in column_names if column_name not in ['Survived','Sex_male','Embarked_S']]\n",
    "\n",
    "# Split into features and responses\n",
    "X = encoded_df[input_features]\n",
    "y = encoded_df[output_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do simple train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try and evaluate KNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is certainly better than the 62% null accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try simple train-test split using different random_state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=3)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try 10-fold cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Initialize KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Calculate and print scores for each fold\n",
    "scores = cross_val_score(knn,X,y,cv=10,scoring='accuracy') #cv=10 means to use 10-fold cross-validation\n",
    "print scores\n",
    "print '\\n'\n",
    "\n",
    "# Print mean\n",
    "print scores.mean() # allegedly more confident this would be closer to actual score on out of sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimize k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "k_range = range(1,31)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn,X,y,cv=10,scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "plt.plot(k_range,k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal value appears to be 3. I wonder if a better value would be 23 (for better variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "print cross_val_score(knn,X,y,cv=10,scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare with logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "print cross_val_score(logreg,X,y,cv=10,scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try different features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = list(encoded_df.columns)\n",
    "output_feature = 'Survived'\n",
    "input_features = ['Sex_female','Fare','Pclass','Embarked_C','Embarked_Q','Age']#[column_name for column_name in column_names if column_name not in ['Survived','Sex_male','Embarked_S']]\n",
    "\n",
    "# Split into features and responses\n",
    "X = encoded_df[input_features]\n",
    "y = encoded_df[output_feature]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "k_range = range(1,31)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn,X,y,cv=10,scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "plt.plot(k_range,k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validation Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=23)\n",
    "print cross_val_score(knn,X,y,cv=10,scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "print cross_val_score(logreg,X,y,cv=10,scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try GridSearch for k-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = list(encoded_df.columns)\n",
    "output_feature = 'Survived'\n",
    "input_features = [column_name for column_name in column_names if column_name not in ['Survived','Sex_male','Embarked_S']]\n",
    "\n",
    "# Split into features and responses\n",
    "X = encoded_df[input_features]\n",
    "y = encoded_df[output_feature]\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "k_range = range(1,31)\n",
    "print k_range\n",
    "\n",
    "# Create parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "print param_grid\n",
    "\n",
    "# Instantiate grid (can set n_jobs = -1 for parallel (if supported on computer/OS))\n",
    "grid = GridSearchCV(knn,param_grid,cv=10,scoring='accuracy')\n",
    "\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print grid.grid_scores_[0].parameters\n",
    "print grid.grid_scores_[0].cv_validation_scores\n",
    "print grid.grid_scores_[0].mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "grid_std_scores = [result.cv_validation_scores.std(ddof=1) for result in grid.grid_scores_]\n",
    "\n",
    "plt.plot(k_range,grid_mean_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tutorial doesn't seem to include error bars\n",
    "yerr=np.array(grid_std_scores)\n",
    "plt.errorbar(k_range,grid_mean_scores,yerr,linestyle='None',marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examine best model\n",
    "print grid.best_score_\n",
    "print grid.best_params_\n",
    "print grid.best_estimator_ # Actual best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search over multiple parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_range = range(1,31)\n",
    "weight_options = ['uniform','distance'] # Default is weight equally in neighborhood. Distance weighs closer neighbors highher\n",
    "\n",
    "param_grid = dict(n_neighbors=k_range,weights=weight_options)\n",
    "print param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(knn,param_grid,cv=10,scoring='accuracy')\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print grid.best_score_\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model with all of data and best parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=8,weights='distance')\n",
    "knn.fit(X,y)\n",
    "\n",
    "knn.predict([3,5,4,2,1,5,6,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare with logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "# print cross_val_score(logreg,X,y,cv=10,scoring='accuracy').mean()\n",
    "param_grid = {}\n",
    "grid = GridSearchCV(logreg,param_grid,cv=10,scoring='accuracy')\n",
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print grid.best_score_\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best classification accuracy so far over the null accuracy of %62."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-search over features as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trimmed_column_names = [column_name for column_name in column_names if column_name not in ['Survived','Sex_male','Embarked_S']]\n",
    "trimmed_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate all combos of features\n",
    "import itertools\n",
    "feature_combos = []\n",
    "for L in range(0, len(trimmed_column_names)+1):\n",
    "    for subset in itertools.combinations(trimmed_column_names, L):\n",
    "        feature_subset = []\n",
    "        if subset:\n",
    "            for thing in subset:\n",
    "                if thing:\n",
    "                    feature_subset.append(thing)\n",
    "        \n",
    "        if feature_subset:\n",
    "            feature_combos.append(feature_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_range = range(1,31)\n",
    "weight_options = ['uniform','distance'] # Default is weight equally in neighborhood. Distance weighs closer neighbors highher\n",
    "feature_combos = feature_combos\n",
    "\n",
    "param_grid = dict(n_neighbors=k_range,feature_combos = feature_combos)\n",
    "print param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "kbest = SelectKBest(f_classif)\n",
    "pipeline = Pipeline([('kbest', kbest), ('lr', LogisticRegression())])\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'kbest__k': [1,2,3,4,5,6,7,8], 'lr__C': np.logspace(-10, 10, 5)},\n",
    "                           cv=10,\n",
    "                          scoring='accuracy')\n",
    "print grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit pipeline with best parameters obtained from grid search using all data\n",
    "pipeline.set_params(**grid_search.best_params_).fit(X, y)\n",
    "\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat_pipe = confusion_matrix(y_test, y_pred_pipe)\n",
    "print conf_mat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "5*4.77*8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "kbest = SelectKBest(f_classif)\n",
    "pipeline = Pipeline([('kbest', kbest), ('svm', SVC())])\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           {'kbest__k': [1,2,3,4,5,6,7,8],'svm__C': [np.log(-10),np.log(-5)]},\n",
    "                           cv=10, scoring='accuracy',n_jobs=-1) #np.logspace(-10, 10, 5)\n",
    "# Print grid_search parameters\n",
    "print grid_search\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Perform grid serach using above parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction based on model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print out metrics and best parameters\n",
    "print 'Confusion matrix: \\n\\n',conf_mat\n",
    "\n",
    "print '\\nNormalized confusion matrix: \\n\\n',conf_mat/float(conf_mat.sum())\n",
    "\n",
    "print '\\nClassification report: \\n\\n',classification_report(y_test, y_pred)\n",
    "\n",
    "print '\\nTraining set classification accuracy: ',grid_search.best_score_\n",
    "print '\\nTest set classification accuracy: ',conf_mat.trace()/float(conf_mat.sum())\n",
    "print '\\nBest parameters:\\n'\n",
    "print grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... similar to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at confusion matrix, classification report, and ROC curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print confusion_matrix(y_test, y_pred)\n",
    "print classification_report(y_test, y_pred)\n",
    "\n",
    "y_pred_prob = grid_search.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred_prob)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.title('ROC curve for diabetes classifier')\n",
    "plt.xlabel('False Positive Rate (1-Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity or True Positive Rate (percent of those classified correctly that are class 1) versus False Positive Rate (percent of those in class 0 incorrectly classified as class 1)\n",
    "\n",
    "**Visualize probabiliy of Survival for test population**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search.predict_proba(X_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_pred_prob,bins=12)\n",
    "plt.xlim(0,1)\n",
    "plt.title('Histogram of predicted probabilities')\n",
    "plt.xlabel('Predicted probability of Survival')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify decision parameter of model to reduce people falsly classified as **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "y_pred_class = binarize(y_pred_prob,0.3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print confusion_matrix(y_test, y_pred)\n",
    "print classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "print confusion_matrix(y_test, y_pred_class)\n",
    "print classification_report(y_test, y_pred_class)\n",
    "\n",
    "# y_pred_prob = grid_search.predict_proba(X_test)[:,1]\n",
    "\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred_prob)\n",
    "# plt.plot(fpr,tpr)\n",
    "# plt.xlim([0.0,1.0])\n",
    "# plt.ylim([0.0,1.0])\n",
    "# plt.title('ROC curve for diabetes classifier')\n",
    "# plt.xlabel('False Positive Rate (1-Specificity)')\n",
    "# plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "# plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work on more advanced visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'll make a copy of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trimmed_df = df.copy()\n",
    "\n",
    "del trimmed_df['Name']\n",
    "del trimmed_df['Ticket']\n",
    "del trimmed_df['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trimmed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def infer_feature_types(df,suppress_report=False):\n",
    "    \"\"\"\n",
    "    Returns dictionary of features and corresponding datatypes \n",
    "    (Either 'category' or 'numerical') given a dataframe\n",
    "    \n",
    "    This can easily misclassify integers, since they can represent \n",
    "    either counts or categories. Additionally, one should try to \n",
    "    avoid using floats as integers.\n",
    "    \n",
    "    A summary is printed at the end so the user can manually modify\n",
    "    the resulting dictionary.        \n",
    "    \"\"\"\n",
    "    # Initialize output dictionary\n",
    "    data_types = {}\n",
    "\n",
    "    # Consider each feature\n",
    "    for feature in list(df.columns):\n",
    "        # Obtain unique feature values\n",
    "        unique_feature_values = list(set(df[feature]))\n",
    "        \n",
    "        # Obtain unique feature types\n",
    "        unique_feature_types = list(set(type(unique_item) for unique_item in unique_feature_values))\n",
    "\n",
    "        # Check for mixed types within feature\n",
    "        if len(unique_feature_types) != 1:\n",
    "            raise Exception(\"Mixed types in feature '%s' encountered. This is presently unsupported.\"%(feature))\n",
    "\n",
    "        # Check that all input types are expected\n",
    "        unique_feature_type = unique_feature_types[0]\n",
    "        if not (unique_feature_type is not str or unique_feature_type is not int or unique_feature_type is not np.int64 or unique_feature_type is not float or unique_feature_type is not np.float64):\n",
    "            raise Exception(\"Feature '%s' is not of type str, int, numpy.int64, float, or numpy.float64. Its listed feature is %s This is currently unsupported\"%(feature,unique_feature_type))\n",
    "        \n",
    "        # Ignore features that appear to be ids, though warn user\n",
    "        if unique_feature_type is str or unique_feature_type is int or unique_feature_type is np.int64:\n",
    "            if len(unique_feature_values) != len(df[feature]):\n",
    "                data_types[feature] = 'category'\n",
    "            else:\n",
    "                if not suppress_report:\n",
    "                    print \"WARNING: feature '%s' appears to be an id and will not be included in the plot\"%(feature)\n",
    "        else:\n",
    "            data_types[feature] = 'numerical'\n",
    "    \n",
    "    # Print report\n",
    "    if not suppress_report:\n",
    "        print ''\n",
    "        print 40*'-'\n",
    "        print 'Data type\\tFeature'\n",
    "        print 40*'-'    \n",
    "        for key in data_types:\n",
    "            print '%s\\t%s'%(data_types[key],key)\n",
    "        print 40*'-'\n",
    "    \n",
    "    # Return\n",
    "    return data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dir(df['Embarked'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(df['Embarked'].value_counts().index.values)\n",
    "print df['Embarked'].value_counts().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(dataframe_visualization_lib)\n",
    "data_types = infer_feature_types(df,suppress_report=True)\n",
    "dataframe_visualization_lib.compare_data(trimmed_df,data_types=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_label_feature = 'Pclass'\n",
    "output_label_feature = 'Embarked'\n",
    "output_labels = ['A','B','C']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "plot_label_versus_label(df,input_label_feature,output_label_feature,output_labels=output_labels,\n",
    "                            colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_types = infer_feature_types(df,suppress_report=True)\n",
    "compare_data(trimmed_df,data_types=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataframe_visualization_lib\n",
    "reload(dataframe_visualization_lib)\n",
    "from dataframe_visualization_lib import compare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_types = infer_feature_types(df,suppress_report=True)\n",
    "compare_data(trimmed_df,data_types=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a histogram of normally distributed random numbers and plot the\n",
    "analytic PDF over it\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "mu, sigma = 100, 15\n",
    "x = mu + sigma * np.random.randn(10000)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax.hist(x, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "\n",
    "# hist uses np.histogram under the hood to create 'n' and 'bins'.\n",
    "# np.histogram returns the bin edges, so there will be 50 probability\n",
    "# density values in n, 51 bin edges in bins and 50 patches.  To get\n",
    "# everything lined up, we'll compute the bin centers\n",
    "bincenters = 0.5*(bins[1:]+bins[:-1])\n",
    "\n",
    "ax.set_xlabel('Smarts')\n",
    "ax.set_ylabel('Probability')\n",
    "#ax.set_title(r'$\\mathrm{Histogram\\ of\\ IQ:}\\ \\mu=100,\\ \\sigma=15$')\n",
    "ax.set_xlim(40, 160)\n",
    "ax.set_ylim(0, 0.03)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trimmed_df['Embarked'] = pd.Categorical(trimmed_df['Embarked'])\n",
    "trimmed_df['Embarked'] = trimmed_df['Embarked'].cat.codes\n",
    "\n",
    "trimmed_df['Sex'] = pd.Categorical(trimmed_df['Sex'])\n",
    "trimmed_df['Sex'] = trimmed_df['Sex'].cat.codes\n",
    "\n",
    "trimmed_df['Pclass'] = pd.Categorical(trimmed_df['Pclass'])\n",
    "trimmed_df['Pclass'] = trimmed_df['Pclass'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trimmed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "continuous_pair_grid_vs_label(trimmed_df,hue_feature='Survived',\n",
    "                              plot_medians=True,\n",
    "                              plot_vars=['Survived','Embarked','Pclass','Age','Fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ways to make these pairgrid plots more informative:\n",
    "\n",
    "- Make it so that diagonals specify the overall distribution\n",
    "  - Could maybe fit distributions\n",
    "- Continuous vs coninuous plots will still be scatter plots\n",
    "- Category vs continous will be distributions colored by category\n",
    "- Category 1 vs category 2 will be distribution of category 2 with each divided into colored proportions of category 1\n",
    "- This leaves continuous vs category?\n",
    "  - Maybe some sort of correlation/covariance metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_label_feature = 'Pclass'\n",
    "output_label_feature = 'Embarked'\n",
    "output_labels = ['A','B','C']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = ''\n",
    "\n",
    "plot_label_versus_label(df,input_label_feature,output_label_feature,output_labels=output_labels,\n",
    "                            colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_label_feature = 'Embarked'\n",
    "output_label_feature = 'Pclass'\n",
    "output_labels = ['1','2','3']\n",
    "colors = ['grey','orange','red']\n",
    "x_label = 'Number of passengers'\n",
    "\n",
    "plot_label_versus_label(df,input_label_feature,output_label_feature,output_labels=output_labels,\n",
    "                            colors=colors,x_label=x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
